
# Autogenerated using jinja files
from quixstreams import Application
from quixstreams.kafka import ConnectionConfig


from river import linear_model
from river import forest
from river import cluster
from river import drift
from river import ensemble
from river import neural_net
from river import preprocessing
from river import time_series
from river import misc
    
from river import tree
    
from river import metrics



#Define models


model2 = linear_model.ALMAClassifier(
    test = 1,
    lr = 0.1,
    dict = {"name" : [1,2]},
    name = 'HELLOITSME',
    test2 = ['1',2]
)
model3 = forest.ARFClassifier(
    optim = model2
)
model4 = cluster.KMeans(
)
model5 = drift.ADWIN(
)
model6 = ensemble.BaggingClassifier(
)
model7 = neural_net.MLPRegressor(
)
model8 = preprocessing.StandardScaler(
)
model9 = time_series.HoltWinters(
)
model10 = misc.Skyline(
)
model11 = tree.splitter.DynamicQuantizer(
)
model12 = metrics.multioutput.ExactMatch(
)



#Define connection
connectionConfig = ConnectionConfig(
    bootstrap_servers = 'localhost:39092',
    security_protocol = 'plaintext',
)

#Connection to Kafka
app = Application( 
    broker_address = connectionConfig,
    consumer_group = 'linear_models',
    auto_offset_reset = 'earliest',
)

#Input topics 

input_topic_testdata1 = app.topic("tester_topic", value_deserializer="json")
input_topic_testdata2 = app.topic("tester_topic", value_deserializer="json")

# Create Streaming DataFrames connected to the input Kafka topics

sdf_testdata1 = app.dataframe(topic=input_topic_testdata1)
sdf_testdata2 = app.dataframe(topic=input_topic_testdata2)


#Drop Features

sdf_testdata1.drop(["drop1"])


#Keep Features

sdf_testdata1 = sdf_testdata1[["keep1","keep2"]]

sdf_testdata2 = sdf_testdata2[["keep1","keep2"]]


#TODO: Assignments 

#Connect composers with preprocessors 

preprocessor_testdata1 =model2|model2


preprocessor_testdata2 =model3+model2|model5

