# Autogenerated from python.template file

from quixstreams import Application
from quixstreams.models import TopicConfig
from quixstreams.kafka import ConnectionConfig 
from pipeline import * 

from river import preprocessing

from river import compose

from river import optim

from river import metrics

from river import drift
from river import linear_model



#Define optimizers
optim1 = optim.AdaDelta()



#Define preprocessors
preproc1 = preprocessing.AdaptiveStandardScaler(
    lr =1,
    optim =optim1,
    param =0.2,
    test =['model', 'model2'],
    dict ={"true" : 1,"false" : 0},
    problem =False,
    string ="stringgg")
preproc2 = preprocessing.FeatureHasher(
    n_features =10,
    seed =42)



#Define composers
selectorNum = compose.SelectType(
    types =(int))
selectorStr = compose.SelectType(
    types =(str))



#Define metrics
testMetric1 = metrics.AdjustedRand()
testMetric2 = metrics.CohenKappa()


#Define live data algorithms
testAlgo = drift.binary.DDM()
testAlgo1 = linear_model.ALMAClassifier()


#Connection Configuration for quixstreams
connectionConfig = ConnectionConfig( 
    
    broker ="localhost:39092",
    connection_type ="sasl_plaintext",
    username ="username",
    password ="admin_pass")

#Connection to Kafka 
app = Application( 
    broker_address = connectionConfig,
    consumer_group ="the_test_consumer_group")

#Input topics 

input_topic_testData = app.topic("test_input_topic", value_deserializer="json")
input_topic_testData2 = app.topic("test_input_topic", value_deserializer="json")

# Create Streaming DataFrames connected to the input Kafka topics

sdf_testData = app.dataframe(topic=input_topic_testData)
sdf_testData2 = app.dataframe(topic=input_topic_testData2)

#FIXME: The below code creates new features in a generic sdf which we might not want 
# See how to fix that 
# Define new features
sdf["generated1"]=((sdf["keep1"])-(2*sdf["keep2"]))
sdf["generated2"]=((sdf["keep"]*sdf["keep"]))
sdf["generated12"]=((sdf["keep1"])-(2*sdf["keep2"]))
sdf["generated22"]=((sdf["keep"]*sdf["keep"]))


#Drop Features

sdf_testData.drop(["drop1","drop2"])


#Keep Features

sdf_testData2 = sdf_testData2[["keep1","keep2"]]


#Connect composers with preprocessors 
preprocessor_testData =preproc1|preproc2
preprocessor_testData2 =selectorNum|preproc1+preproc2


#Pipeline definition 

testPipeline_pipeline = preprocessor_testData | testAlgo

testPipeline_metrics = [testMetric1,testMetric2]
testPipeline = Pipeline(model = testPipeline_pipeline , metrics_list = testPipeline_metrics , name = "testPipeline",output_topic="tester_topic")

testPipeline1_pipeline = preprocessor_testData2 | testAlgo1

testPipeline1_metrics = [testMetric1,testMetric2]
testPipeline1 = Pipeline(model = testPipeline1_pipeline , metrics_list = testPipeline1_metrics , name = "testPipeline1",output_topic="tester_topic")

# Output topics initialization

output_topic_testPipeline = app.topic(testPipeline.output_topic, value_deserializer="json")

output_topic_testPipeline1 = app.topic(testPipeline1.output_topic, value_deserializer="json")


#Sdf for each pipeline 
#Train and predict method calls for each pipeline
#If the pipeline has an output topic then we call it 

sdf_testPipeline = sdf_testData.apply(testPipeline.train_and_predict()).to_topic(testPipeline.output_topic)
sdf_testPipeline1 = sdf_testData2.apply(testPipeline1.train_and_predict()).to_topic(testPipeline1.output_topic)


# Run Quix Streams 
app.run()

#Metric plots for each Pipeline
testPipeline.metrics_plot()

testPipeline1.metrics_plot()

