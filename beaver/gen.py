# Autogenerated from python.template file

from quixstreams import Application
from quixstreams.models import TopicConfig
from quixstreams.kafka import ConnectionConfig 


from river import preprocessing

from river import optim

from river import metrics

from river import drift
from river import linear_model



#Define optimizers
optim1 = optim.AdaDelta()



#Define preprocessors
preproc1 = preprocessing.AdaptiveStandardScaler(
    lr =1,
    optim =optim1,
    param =0.2,
    test =['model', 'model2'],
    dict ={"true" : 1,"false" : 0},
    problem =False,
    string ="stringgg")
preproc2 = preprocessing.FeatureHasher(
    n_features =10,
    seed =42)



#Define metrics
testMetric1 = metrics.AdjustedRand()
testMetric2 = metrics.CohenKappa()


#Define live data algorithms
testAlgo = drift.binary.DDM()
testAlgo1 = linear_model.ALMAClassifier()


#Connection Configuration for quixstreams
connectionConfig = ConnectionConfig( 
    
    broker ="localhost:39092",
    connection_type ="sasl_plaintext",
    username ="username",
    password ="admin_pass")

#Connection to Kafka 
app = Application( 
    broker_address = connectionConfig,
    consumer_group ="the_test_consumer_group")

#Input topics 

input_topic_testData = app.topic("test_input_topic", value_deserializer="json")
input_topic_testData2 = app.topic("test_input_topic", value_deserializer="json")

# Create Streaming DataFrames connected to the input Kafka topics

sdf_testData = app.dataframe(topic=input_topic_testData)
sdf_testData2 = app.dataframe(topic=input_topic_testData2)

#FIXME: The below code creates new features in the main sdf which we might not want 
# See how to fix that 
# Define new features
sdf["generated1"]=((sdf["keep1"])-(2*sdf["keep2"]))
sdf["generated2"]=((sdf["keep"]*sdf["keep"]))
sdf["generated12"]=((sdf["keep1"])-(2*sdf["keep2"]))
sdf["generated22"]=((sdf["keep"]*sdf["keep"]))

